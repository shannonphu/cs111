Shannon Phu
404403562

My SortedList implementation stayed relatively constant to Project 2B's implementation.
My lab2c.c basically followed my lab2b.c, but I had to change how my nodes and head of
lists were initially stored before being passed to threads to process. I had to
dynamically allocate them since depending on whether --list was input as an argument
I had to dynamically make an array of pointers to nodes/heads of lists. I also had to
implement a simple hashing function to determine which list a particular key would
insert into.


QUESTION 2C.1A
Explain the change in performance of the synchronized methods as a function of the number of threads per list.
As the number of threads increases per list, that means more threads are
modifying the same list at once. There is a higher frequency of threads
modifying the same list so with locking mechanisms, the critical sections
will be locked more frequently and threads will be waiting more often
for access. Spin-lock time per operation grows faster than mutex's since
a spin-lock makes it so that there's more pointless waiting for other threads.

QUESTION 2C.1B
Explain why threads per list is a more interesting number than threads (for this particular measurement).
If there were 1 thread per list, then all linked list operations could be
done in parallel, and that would not contribute to the overhead of accessing
shared memory and responding to multiple threads in a single critical section.
If we increase the relative number of threads per list, we are increasing the
frequency of locking which is what we want to analyze in this project.

gprof Performance Analysis:

Unprotected:
Time is most spend in SortedList_insert and SortedList_lookup.
An example gprof output when run with 10,000 iterations and
3 threads is:

  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 61.64      1.33     1.33    29845     0.04     0.04  SortedList_insert
 36.61      2.12     0.79    29749     0.03     0.03  SortedList_lookup
  0.93      2.14     0.02    30000     0.00     0.00  randomString
  0.93      2.16     0.02        3     6.67   714.10  doOperations
  0.00      2.16     0.00    59626     0.00     0.00  hashKeyToList
  0.00      2.16     0.00    30001     0.00     0.00  makeNode
  0.00      2.16     0.00    29867     0.00     0.00  SortedList_delete
  0.00      2.16     0.00        4     0.00     0.00  SortedList_length
  0.00      2.16     0.00        1     0.00     0.00  freeMemory

And for 5000 iterations and 2 threads:
  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 66.74      0.14     0.14     9922     0.01     0.01  SortedList_insert
 33.37      0.21     0.07     9925     0.01     0.01  SortedList_lookup
  0.00      0.21     0.00    19853     0.00     0.00  hashKeyToList
  0.00      0.21     0.00    10001     0.00     0.00  makeNode
  0.00      0.21     0.00    10000     0.00     0.00  randomString
  0.00      0.21     0.00     9969     0.00     0.00  SortedList_delete
  0.00      0.21     0.00        3     0.00     0.00  SortedList_length
  0.00      0.21     0.00        2     0.00   105.11  doOperations
  0.00      0.21     0.00        1     0.00     0.00  freeMemory

The number of calls for the main function calls which take of the majority
of the programs runtime is equal to the number of iterations * number of
threads. For example in the above example with --iterations=5000 and
--threads=2, the number of calls to SortedList_insert, SortedList_delete
and SortedList_lookup, is approximately 10,000.

SortedList_insert and SortedList_lookup seem to take the most time, with
SortedList_insert taking the majority.

For mutex synchronization:

Generally the time distribution looks like this
  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 50.05      0.01     0.01    17957     0.00     0.00  hashKeyToList
 50.05      0.02     0.01     8566     0.00     0.00  SortedList_lookup
  0.00      0.02     0.00    10050     0.00     0.00  makeNode
  0.00      0.02     0.00    10000     0.00     0.00  randomString
  0.00      0.02     0.00     9008     0.00     0.00  SortedList_insert
  0.00      0.02     0.00     8980     0.00     0.00  SortedList_delete
  0.00      0.02     0.00      150     0.00     0.00  SortedList_length
  0.00      0.02     0.00       50     0.00     0.00  freeMemory
  0.00      0.02     0.00        1     0.00     0.00  deinitMutex
  0.00      0.02     0.00        1     0.00    20.02  doOperations
  0.00      0.02     0.00        1     0.00     0.00  initMutex

where hashKeyToList and SortedList_lookup take up the most time, evenly at
50%-50%. In the above example, the arguments were --iterations=5000
--threads=2 and --list=50. hashKeyToList was called about 20,000 times
which is correct because before inserting and looking up, I had to call
that function to look into what list to look for the node in. makeNode
and randomString were accurately called about 10,000 times for the number
of nodes created. hashKeyToList and SortedList_lookup each took up the
majority of the program time, both at 50%.

In the case of --iterations=10000 --threads=2 and --list=10
  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 68.82      0.11     0.11    19498     0.01     0.01  SortedList_insert
 31.28      0.16     0.05    19337     0.00     0.00  SortedList_lookup
  0.00      0.16     0.00    38906     0.00     0.00  hashKeyToList
  0.00      0.16     0.00    20010     0.00     0.00  makeNode
  0.00      0.16     0.00    20000     0.00     0.00  randomString
  0.00      0.16     0.00    19687     0.00     0.00  SortedList_delete
  0.00      0.16     0.00       30     0.00     0.00  SortedList_length
  0.00      0.16     0.00       10     0.00     0.00  freeMemory
  0.00      0.16     0.00        2     0.00    80.09  doOperations
  0.00      0.16     0.00        1     0.00     0.00  deinitMutex
  0.00      0.16     0.00        1     0.00     0.00  initMutex

The insert function took longest at 68% followed by lookup at 31%.

For spin-lock synchronization:

The time distribution generally looks like
  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 52.38      0.90     0.90    84467     0.01     0.01  spin_lock
 26.77      1.36     0.46    40873     0.01     0.01  SortedList_insert
 18.62      1.68     0.32    42207     0.01     0.01  SortedList_lookup
  1.16      1.70     0.02    91281     0.00     0.00  hashKeyToList
  0.58      1.71     0.01    50000     0.00     0.00  randomString
  0.58      1.72     0.01        2     5.01   855.92  doOperations
  0.00      1.72     0.00    93131     0.00     0.00  spin_unlock
  0.00      1.72     0.00    50010     0.00     0.00  makeNode
  0.00      1.72     0.00    46400     0.00     0.00  SortedList_delete
  0.00      1.72     0.00       60     0.00     0.00  SortedList_length
  0.00      1.72     0.00       10     0.00     0.00  freeMemory

This above was run with --iterations=10000 --threads=5 --list=10. Most of the
time is spent on spinning the lock, over 50%. The number of calls to spin
the lock were more than the number of nodes.

But when there's more lists like if --list=50, the time distribution is

  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 36.71      0.11     0.11    33753     0.00     0.00  SortedList_lookup
 30.03      0.20     0.09    36629     0.00     0.00  SortedList_insert
 13.35      0.24     0.04    70537     0.00     0.00  spin_lock
  6.67      0.26     0.02    72012     0.00     0.00  hashKeyToList
  6.67      0.28     0.02    50000     0.00     0.00  randomString
  3.34      0.29     0.01      292     0.03     0.03  SortedList_length
  3.34      0.30     0.01        2     5.01   139.29  doOperations
  0.00      0.30     0.00    75363     0.00     0.00  spin_unlock
  0.00      0.30     0.00    50050     0.00     0.00  makeNode
  0.00      0.30     0.00    36595     0.00     0.00  SortedList_delete
  0.00      0.30     0.00       50     0.00     0.00  freeMemory

where the majority of time is spent in lookup and insert instead of spinning
the lock. The number of calls to spin the lock were less if there are more
lists.

QUESTION 2C.2A
Compare the time per operation when increasing the lists value. Explain your observations.
As my number of lists increases, the time it takes per operation decreases.
This is the trend for unprotected, mutex, and spin-lock. The time decreases
because first the lists become shorter as more lists are created, but the
number of nodes inserted remain constant. Thus it takes less time to search
through the list for the correct location to insert. Also for locked implementation,
there are more threads and more different locks to avoid waiting for the
lock on that list. There are more lists so less waiting occurs.

QUESTION 2C.2B
Compare the time per operation between mutex and spinlock. Explain your observations.
With larger ratio of threads to lists, the spin-lock takes longer to run the program
compared to the mutex implementation. This is because spin waiting is less performant
than a mutex when it comes to waiting for locks to be released in a given critical
section. As the ratio becomes more skewed towards more threads than lists, the
effects of the spin wait become stronger than the effects of a mutex.

QUESTIONS 2C.3A
Why must the mutex be held when pthread_cond_wait is called?
The mutex must be held because the thread that is signaled later after wait is called
will receive the lock and continue execution. Multiple threads can queue up waiting for
possession of the lock when another thread signals the condition variable.
If the mutex isn't held, there could be a race condition where another thread
could change the state of the program so that pthread_cond_signal sends a signal, but no
threads are waiting. So thus the waiting thread will continue waiting forever.

QUESTION 2C.3B
Why must the mutex be released when the waiting thread is blocked?
If the waiting thread is blocked, the mutex must be released to prevent a deadlock from
occurring. If the blocked thread is still holding a lock, then maybe some other thread
could need to access that critical section and hold that lock. But if the waiting thread
is blocked, waiting for some other signal to occur, if it so happens that the second
thread waiting for the lock is the one who has to provide the first thread the resource,
then deadlock will occur since the 2 threads are waiting for each other's resources.

QUESTION 2C.3C
Why must the mutex be reacquired when the calling thread resumes?
The mutex is reacquired to prevent race conditions after the thread resumes.
Upon the call to wait, the lock is assume to be held by the calling thread.
So on return, it is expected that the thread should stil hold the lock
to maintain mutual exclusivity in that particular critical section and avoid
race conditions.


QUESTION 2C.3D
Why must mutex release be done inside of pthread_cond_wait?  Why can't the caller simply release the mutex before calling pthread_cond_wait?
If the mutex were released before calling wait, there could be the risk of a thread context
switch right before the thread goes to sleep and starts waiting. The switch could be to the
thread who calls signal to the sleeping thread, but signal won't wake up the waiting thread
since it's not sleeping yet. Thus when the thread who calls signal returns, the original
thread resumes execution and starts waiting, but no signal will ever wake it up.

QUESTION 2C.3E
Can pthread_cond_wait be implemented in user mode?  If so, how?  If it can only be implemented by a system call, explain why?
It can be implemented in user mode, but the implementaton would be buggy, and prone
to race condition caused by interrupts. It could be implemented by disabling interrupts.
The OS should manage the implementation since it will ensure the safety of the critical
section so that the code section will execute atomically. A system call can ask the OS
to request for the hardware to handle the implementation of locks. Atomicity needs
hardware support as well as OS support to make the locking correct.